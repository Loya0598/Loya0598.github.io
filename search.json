[{"title":"Transformer","url":"/2020/07/03/Transformer/","content":"\n## The Illustrated Transformer文章阅读\n\n### 网页链接：http://jalammar.github.io/illustrated-transformer/\n\n### 阅读时间：2020.07.03\n\n---\n\n### 1. Simple Intro\n\n- A model that uses attention to boost the speed\n- Outperforms the **Google Neural Machine Translation model**\n- Parallelization\n\n### 2. A High-Level Look\n\n- Sentence1 $\\rightarrow$ **Model** $\\rightarrow$ Sentence2\n\n- Sentence1 $\\rightarrow$ **Encoder** $\\rightarrow$ **Decoder** $\\rightarrow$ Sentence2\n\n- Sentence1 $\\rightarrow$ **Stack(Encoder, 6)** $\\rightarrow$ **Stack(Decoder, 6)** $\\rightarrow$ Sentence2\n\n  - Encoders don't share weights\n\n- Sentence1 $\\rightarrow$ **Stack(Self-Attention + FFNN, 6)** $\\rightarrow$ **Stack(Self-Attention + Encoder-Decoder Attention + FFNN, 6)** $\\rightarrow$ Sentence2\n\n  > Question1: The exact same feed-forward network is independently applied to each position.\n  >\n  > Answer1: For each position, FFNN is the same, so it can be done in parallel.\n\n### 3. Bringing The Tensors Into The Picture\n\n- Encoders\n\n  - Encoder Structure\n\n    - Input: [Batch_Size, Seq_Len, Hidden_Dim1]\n    - Layers: Self-Attention Layer + feed forward Layer\n    - Output: [Batch_Size, Seq_Len, Hidden_Dim2]\n\n  - Self-Attention:\n\n    - Concept: Using the representation of other positions to encode the word that is processing now.\n\n    - Details:\n\n      - Input Vector $x$ / Query Vector $q$ / Key Vector $k$ / Value Vector $v$\n\n      - First Step:\n\n        - Query Vector $q = x * W^Q$\n\n        - Key Vector $k = x * W^K$\n\n        - Value Vector $v = x * W^V$\n\n        - $W^Q, W^K, W^V$ are the parameters trained during the training process\n\n          >Dimension of new vectors: 64\n          >\n          >Dimension of input vectors: 512\n          >\n          >They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant\n\n      - Second Step:\n\n        - Score: When processing the word in position i, the score at position j is **Query Vector $q_i$ * Key Vector $k_j$*** (dot product)\n\n      - Third Step:\n\n        - divide the scores by $d$, $d = \\sqrt{d_k}$\n\n      - Forth Step:\n\n        - Score $\\rightarrow$Softmax Layer\n\n      - Fifth Step:\n\n        - Softmax Score * Value vector\n\n      - Sixth Step:\n\n        - Sum up the weighted value vectors\n\n    - Matrix Form:\n\n      - Input Data: $X$\n      - Query Vector: $Q = XW^Q$\n      - Key Vector: $K = XW^K$\n      - Value Vector: $V = XW^V$\n      - Output: $Z = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n\n    - \"Many Heads\"\n\n      - Improvements:\n        - Expands the model's ability to focus on different positions\n        - Gives the attention layer multiple \"representation subspaces\"\n          - Using multiple sets of Query/Key/Value weight matrices\n      - Implementation:\n        - How to condense multiple outputs:\n          - Concatenate and multiply them by an additional weight matrix $W^O$\n          - Formula: $O = Concat(Z) * W^O$\n\n    > Question2: If we add all the attention heads to the picture, however, things can be harder to interpret, So how to interpret this weights attention score?\n    >\n    > Answer2: Interpret the weight of the head one by one?\n\n### 4. Representing The Order of The Sequence Using Positional Encoding\n\n- Aim: To give the model a sense of the order of the words\n- Method: Add positional encoding vectors\n- Implementation: \n\n> Question3: How to implement position encoding? list some ideas\n\n### 5. The Residuals\n\n- Each sub-layer(Self-Attention, FFNN) has a residual connection around it. And it is followed by a layer-normalization step\n- Formula: $Output = LayerNorm(X + Z)$\n\n### 6. The Decoder Side\n\n- Self Attention Layer\n\n  - Aim: give attention on the earlier positions in the output sequence\n  - Implementation: mask future position before the softmax step\n\n  > Question4: How to implement it in parallel, it looks like it can only process one word at each forward step.\n  >\n  > Answer4: Training - parallel\n\n- Encoder-Decoder Attention Layer\n\n  - Using the output of the top encoder to calculate the attention weight of input\n  - Formula: $K = XK_{enc\\_dec}$, $V = XV_{enc\\_dec}$\n\n  > Question5: What's the query vector?\n  >\n  > Answer5: From the layer below it\n\n- FFNN\n\n- Output Layer: Linear + Softmax\n\n### 7. The Loss Function\n\n- Selection: Cross Entropy / KL divergence\n- Output Mode: Greedy Decoding / Beam Search\n\n\n\n\n\n\n\n\n\n\n\n","tags":["paper notes"]},{"title":"剪网","url":"/2020/06/19/剪网/","content":"\n> 2020.06.20 00：38躺床上躺了近一个小时 睡不着 起来也写不动代码 就决定把这篇前天看的文章的读书笔记补了\n>\n> 这是第二次读缘缘堂，上次没有读完，书还搁在学校床头，是从一个毕业学长那拿回来的，这回618又凑单买了一本，这次的封面格外精巧，摸起来很舒服，小小本，适合睡前看。\n\n这个版本第一篇是《剪网》，从大娘舅说的那句“白相真开心，但是一想起铜钱就不开心”引入，在谈事物与事物关系。学了一个新词，叫白相，吴语里指玩耍。\n\n> \"原来‘价钱’的一种东西，容易让人限制又减小事物的意义\"\n>\n> “故我们倘要认识事物的本身的存在的真意义，就非撤去其对世间的一切关系不可”\n>\n> “大娘舅一定能够常常不想起铜钱而白相大世界，所以能这样开心而赞美。然而他只是撤去‘价钱’的一种关系而已。倘能常常不想起世间一切的关系而在这世界里做人，其人生一定更多欢慰”\n>\n> ”我仿佛看见这世间有一个极大而极复杂的网，大大小小的事物，都被牢结爱这网中，所以我想把握某一种事物的时候，总要牵动无数的线，带出无数的别的事物，使得本物不能孤独地明晰地显现在我的眼前，因之永远不能看见世界的真相“\n>\n> ”大娘舅在大世界里，只将其与‘钱’相结的一根线剪断，已能得到满足而归来。所以我想找一把快剪刀，把这个网尽行剪破，然后来认识这世界的真相。“\n\n昨天读完，觉得很矛盾，因为在我的认知中，事物和事物关系是不可分割的，我们很难剖开二者去认识其中一个，一个事物是好或者一个事物是坏皆是由它和它周围的事物关系判断得到，并且这种判断还常常带有主观性。撤去一切关系后，我们又要从何角度去认识事物。丰子恺先生爱写道理，常常一点小事物，他都会引出一些哲思，我对他写的道理常常只能强行作解，撤去关系想要本物更明晰地展现在他眼前，可能说的是，当我们想从一个角度去认识事物时，我们能清楚的认识到每个事物关系对这个事物的影响，并非真正撤去，而是我们能主观的决定是否考虑某些事物关系对当前事物判断的影响。“钱”让白相的意义变了味，命令式的上课铃下课铃让教课变了味，既然认清了这些，当我们想要说明白相的意义或者教课的意义时，便可从自身决定是否受“钱”或者一些无谓的命令影响。\n\n这篇文章的结尾也是很有意思的一句话话，我还没读懂。\n\n> “艺术，宗教，就是我想找求来剪破这‘世网’的剪刀吧“\n\n","tags":["缘缘堂","丰子恺"]},{"title":"MaskGAN","url":"/2020/06/19/MaskGAN/","content":"\n## MaskGAN\n\n### 论文链接：https://arxiv.org/pdf/1801.07736.pdf\n\n### 论文代码：https://github.com/tensorflow/models/tree/master/research/maskgan\n\n### 阅读时间：2020.06.18\n\n------\n\n### 1、Simple Intro\n\n- 他们的任务模式是填空，不再是从头生成完整句子\n- 他们采用了actor-critic的框架去计算每一步的reward\n- 他们采用了一些新的evaluation metrics对他们的模型进行评估\n\n### 2、Architecture\n\n- Generator\n\n  - Model: $G(x_t) = P(\\hat{x}_t | \\hat{x}_1, ..., \\hat{x}_\n    {t - 1}, m(x))$\n  - 通过语言模型生成填空后的句子，比如“I want _ _ apple” 输出 \"I want to pick apple\"\n\n- Discriminator\n\n  - Model: $D_{\\phi}(\\tilde{x_t}|\\tilde{x}_{0:T}, m(x)) = P(\\tilde{x}_t = x_t^{real} | \\tilde{x}_{0:T}, m(x))$\n\n  - 通过同样的语言模型，不再生成句子，而是取最后的分布概率中的正确token的概率，比如输入是\"I want to pick apple\"，真实数据是“I want this fresh apple”，经过语言模型我们会得到输出时每个token的一个分布概率，比如对于第一个mask的空位，得到的分布概率是{to: 0.6，this: 0.2，that: 0.2}，那么discriminator在这个token上的输出既是0.2。\n\n  - Reward: $r_t = logD_{\\phi}(\\tilde{x}_t | \\tilde{x}_{0:T}, m(x))$ 即为概率值取log\n  - Reward at each position: $R_t = \\sum_{s = t}^T \\gamma^sr_s$\n\n### 3、Training\n\n- Generator:\n\n  - Reward at step t: $\\mathbb{E}_G[R_t]$\n    - 因为sample这个操作不可导（不可导的解释可以参照这一篇文章：[Role of RL in text generation by GAN](https://zhuanlan.zhihu.com/p/29168803)），所以我们可以利用RL中的policy gradient，将每一步的sample类比于RL中的采取动作，$\\theta$即为我们的策略参数，我们期望能调整$\\theta$来最大化reward。\n  - Gradient of Reward at step t:\n    - $\\nabla_{\\theta}\\mathbb{E}_{G}[R_t] = \\mathbb{E}_{G}[R_t\\nabla_{\\theta}logG_{\\theta}(\\hat{x}_t)]$\n  - MiniBatch Update at each step(主要是想说明在实际用batch进行更新时loss要如何写):\n    - Loss = $\\frac{1}{N}\\sum_{i = 1}^N(R_tlog(G_{\\theta}(\\hat{x}_t)))$\n    - 因为我们的x可以看成是从G中采样的，所以对于上面那个gradient的期望，我们直接计算后半部分就可以了\n  - Gradient: <img src=\"MaskGAN/maskgan1.png\" style=\"zoom:50%;\" />\n\n- Discriminator:\n\n  - Gradient: <img src=\"MaskGAN/maskgan2.png\" style=\"zoom:50%;\" />\n\n- Critic:\n\n  - $b_t = V^G(x_{1:t})$\n  - to reduce the variance of this gradient estimator\n\n- Alternativa approaches for long sequences and large vocabularies\n\n  - long sequences:\t先在T这个长度将模型训练到收敛，再将其迁移到长度（T+1）上\n  - Large vocabularies: 在生成每个token时，discriminator对每个可能的采样都输出reward，然后利用这个reward相应的去更新generator\n\n  > 这样代码如何实现？\n\n- Training Details:\n\n  > First we train a language model using standard maximum likelihood training. We then use the pretrained language model weights for the seq2seq encoder and decoder modules. \n  >\n  > With these language models, we now pretrain the seq2seq model on the in-filling task using maximum likelihood\n\n### 4、Evaluation & Experiment\n\n- Datasets:\n\n  - The Penn Treebank(PTB)\n\n    >We first pretrain the commonly-used variational LSTM language model with parameter dimensions common to MaskGAN following Gal & Ghahramani (2016) to a validation perplexity of 78. After then loading the weights from the language model into the MaskGAN generator we further pretrain with a masking rate of 0.5 (half the text blanked) to a validation perplexity of 55.3. Finally, we then pretrain the discriminator on the samples produced from the current generator and real training text.\n\n  - IMDB Movie Dataset\n\n    >Identical to the training process in PTB, we pretrain a language model to a validation perplexity of 105.6. After then loading the weights from the language model into the MaskGAN generator we further pretrain with masking rate of 0.5 (half the text blanked) to a validation perplexity of 87.1. Finally, we then pretrain the discriminator on the samples produced from the current generator and real training text.\n\n- Evaluation\n\n  - perplexity\n  - Unique n-grams\n    - 用这个说明mode-collapse的问题\n  - Human Evaluation\n    - <img src=\"MaskGAN/maskgan3.png\" alt=\"alt\" style=\"zoom:50%;\" />\n    - <img src=\"MaskGAN/maskgan4.png\" alt=\"alt\" style=\"zoom:50%;\" />\t\n\n### 5、一些讨论\n\n> 1. 如果从训练就没有mask，那么这个应该就转变成了一个以lstm为discriminator的普通gan，那么mask在其中起到了一个什么作用？\n>\n> 2. mode collapse是通过in-filling这种任务类型解决的？\n>\n> 3. Appendix中提到一个matching syntax at boundaries，这个有什么解决办法吗？\n>\n>    \n\n\n\n\n\n","tags":["paper notes","generative model","GAN"]},{"title":"VAE","url":"/2020/06/18/VAE/","content":"\n## Paper: Auto-Encoding Variational Bayes\n\n### 论文链接：https://arxiv.org/pdf/1312.6114.pdf\n\n### 代码链接：https://github.com/bojone/vae\n\n### 阅读时间：2020.06.17\n\n------\n\n### 一、知识补充\n\n- 泛函：函数的函数；变分问题就是为了求解泛函问题的极值。\n- 题目中的泛函其实指的是KL散度作为loss找特定分布的时候用到了泛函的思想\n\n### 二、VAE 和 普通AutoEncoder的区别\n\n- 首先，我们考虑普通的autoencoder， 它的结构是通过对输入进行编码得到一个中间的隐层向量，然后希望通过解码隐层向量得到的输出能和输入的差别尽量小，通过这种编码解码的结构，以重构的差异作为损失来训练它的网络。\n- 但是这样做存在一个局限，==当我们想运用已经训练好的autoencoder模型时我们无法直接给定中间的隐层向量就让模型输出任意的图片==，因为我们不知道中间隐层向量的分布。\n- 所以VAE就是在encoder这个层面想了一个办法，尽量==让隐层向量的分布符合某个正态分布==，这样当我们已经知道了正态分布的均值和方差之后，我们就可以生成一个中间的隐层向量，然后利用它去生成任意的图片。\n\n### 三、VAE如何做（只考虑Encoder，因为我认为VAE仅在Encoder层面作出了很大改变）\n\n- 目标：P(Z | X) 趋向某种正态分布\n- 方法：\n  - Encoder分为两部分：均值Encoder和方差Encoder（方差这里做了一个log的处理，但对整体分析没有影响，所以暂时不考虑这一块）\n  - Encoder的Loss设计：输入的图片，输入均值和方差，利用KL散度去衡量这个均值和方差与$N(0, 1)$之间的差距。（这个意义上来说，对于任何分布的数据，我们都能将它变换到正态分布？这种理论依据是？）\n- 模型应用：\n  - 在对模型进行训练之后，我们仅需要从正态分布中采样就可以输出任意图片。\n\n","tags":["paper notes","generative model","VAE"]}]