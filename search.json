[{"title":"剪网","url":"/2020/06/19/剪网/","tags":["缘缘堂","丰子恺"]},{"title":"MaskGAN","url":"/2020/06/19/MaskGAN/","content":"\n## MaskGAN\n\n### 论文链接：https://arxiv.org/pdf/1801.07736.pdf\n\n### 论文代码：https://github.com/tensorflow/models/tree/master/research/maskgan\n\n### 阅读时间：2020.06.18\n\n------\n\n### 1、Simple Intro\n\n- 他们的任务模式是填空，不再是从头生成完整句子\n- 他们采用了actor-critic的框架去计算每一步的reward\n- 他们采用了一些新的evaluation metrics对他们的模型进行评估\n\n### 2、Architecture\n\n- Generator\n\n  - Model: $G(x_t) = P(\\hat{x}_t | \\hat{x}_1, ..., \\hat{x}_\n    {t - 1}, m(x))$\n  - 通过语言模型生成填空后的句子，比如“I want _ _ apple” 输出 \"I want to pick apple\"\n\n- Discriminator\n\n  - Model: $D_{\\phi}(\\tilde{x_t}|\\tilde{x}_{0:T}, m(x)) = P(\\tilde{x}_t = x_t^{real} | \\tilde{x}_{0:T}, m(x))$\n\n  - 通过同样的语言模型，不再生成句子，而是取最后的分布概率中的正确token的概率，比如输入是\"I want to pick apple\"，真实数据是“I want this fresh apple”，经过语言模型我们会得到输出时每个token的一个分布概率，比如对于第一个mask的空位，得到的分布概率是{to: 0.6，this: 0.2，that: 0.2}，那么discriminator在这个token上的输出既是0.2。\n\n  - Reward: $r_t = logD_{\\phi}(\\tilde{x}_t | \\tilde{x}_{0:T}, m(x))$ 即为概率值取log\n  - Reward at each position: $R_t = \\sum_{s = t}^T \\gamma^sr_s$\n\n### 3、Training\n\n- Generator:\n\n  - Reward at step t: $\\mathbb{E}_G[R_t]$\n    - 因为sample这个操作不可导（不可导的解释可以参照这一篇文章：[Role of RL in text generation by GAN](https://zhuanlan.zhihu.com/p/29168803)），所以我们可以利用RL中的policy gradient，将每一步的sample类比于RL中的采取动作，$\\theta$即为我们的策略参数，我们期望能调整$\\theta$来最大化reward。\n  - Gradient of Reward at step t:\n    - $\\nabla_{\\theta}\\mathbb{E}_{G}[R_t] = \\mathbb{E}_{G}[R_t\\nabla_{\\theta}logG_{\\theta}(\\hat{x}_t)]$\n  - MiniBatch Update at each step(主要是想说明在实际用batch进行更新时loss要如何写):\n    - Loss = $\\frac{1}{N}\\sum_{i = 1}^N(R_tlog(G_{\\theta}(\\hat{x}_t)))$\n    - 因为我们的x可以看成是从G中采样的，所以对于上面那个gradient的期望，我们直接计算后半部分就可以了\n  - Gradient: ![alt](maskgan1.png)\n\n- Discriminator:\n\n  - Gradient: ![alt](maskgan2.png)\n\n- Critic:\n\n  - $b_t = V^G(x_{1:t})$\n  - to reduce the variance of this gradient estimator\n\n- Alternativa approaches for long sequences and large vocabularies\n\n  - long sequences:\t先在T这个长度将模型训练到收敛，再将其迁移到长度（T+1）上\n  - Large vocabularies: 在生成每个token时，discriminator对每个可能的采样都输出reward，然后利用这个reward相应的去更新generator\n\n  > 这样代码如何实现？\n\n- Training Details:\n\n  > First we train a language model using standard maximum likelihood training. We then use the pretrained language model weights for the seq2seq encoder and decoder modules. \n  >\n  > With these language models, we now pretrain the seq2seq model on the in-filling task using maximum likelihood\n\n### 4、Evaluation & Experiment\n\n- Datasets:\n\n  - The Penn Treebank(PTB)\n\n    >We first pretrain the commonly-used variational LSTM language model with parameter dimensions common to MaskGAN following Gal & Ghahramani (2016) to a validation perplexity of 78. After then loading the weights from the language model into the MaskGAN generator we further pretrain with a masking rate of 0.5 (half the text blanked) to a validation perplexity of 55.3. Finally, we then pretrain the discriminator on the samples produced from the current generator and real training text.\n\n  - IMDB Movie Dataset\n\n    >Identical to the training process in PTB, we pretrain a language model to a validation perplexity of 105.6. After then loading the weights from the language model into the MaskGAN generator we further pretrain with masking rate of 0.5 (half the text blanked) to a validation perplexity of 87.1. Finally, we then pretrain the discriminator on the samples produced from the current generator and real training text.\n\n- Evaluation\n\n  - perplexity\n  - Unique n-grams\n    - 用这个说明mode-collapse的问题\n  - Human Evaluation\n    - ![alt](maskgan3.png)\n    - ![alt](maskgan4.png)\t\n\n### 5、一些讨论\n\n> 1. 如果从训练就没有mask，那么这个应该就转变成了一个以lstm为discriminator的普通gan，那么mask在其中起到了一个什么作用？\n>\n> 2. mode collapse是通过in-filling这种任务类型解决的？\n>\n> 3. Appendix中提到一个matching syntax at boundaries，这个有什么解决办法吗？\n>\n>    \n\n\n\n\n\n","tags":["paper notes","generative model","GAN"]},{"title":"VAE","url":"/2020/06/18/VAE/","content":"\n## Paper: Auto-Encoding Variational Bayes\n\n### 论文链接：https://arxiv.org/pdf/1312.6114.pdf\n\n### 代码链接：https://github.com/bojone/vae\n\n### 阅读时间：2020.06.17\n\n------\n\n### 一、知识补充\n\n- 泛函：函数的函数；变分问题就是为了求解泛函问题的极值。\n- 题目中的泛函其实指的是KL散度作为loss找特定分布的时候用到了泛函的思想\n\n### 二、VAE 和 普通AutoEncoder的区别\n\n- 首先，我们考虑普通的autoencoder， 它的结构是通过对输入进行编码得到一个中间的隐层向量，然后希望通过解码隐层向量得到的输出能和输入的差别尽量小，通过这种编码解码的结构，以重构的差异作为损失来训练它的网络。\n- 但是这样做存在一个局限，==当我们想运用已经训练好的autoencoder模型时我们无法直接给定中间的隐层向量就让模型输出任意的图片==，因为我们不知道中间隐层向量的分布。\n- 所以VAE就是在encoder这个层面想了一个办法，尽量==让隐层向量的分布符合某个正态分布==，这样当我们已经知道了正态分布的均值和方差之后，我们就可以生成一个中间的隐层向量，然后利用它去生成任意的图片。\n\n### 三、VAE如何做（只考虑Encoder，因为我认为VAE仅在Encoder层面作出了很大改变）\n\n- 目标：P(Z | X) 趋向某种正态分布\n- 方法：\n  - Encoder分为两部分：均值Encoder和方差Encoder（方差这里做了一个log的处理，但对整体分析没有影响，所以暂时不考虑这一块）\n  - Encoder的Loss设计：输入的图片，输入均值和方差，利用KL散度去衡量这个均值和方差与$N(0, 1)$之间的差距。（这个意义上来说，对于任何分布的数据，我们都能将它变换到正态分布？这种理论依据是？）\n- 模型应用：\n  - 在对模型进行训练之后，我们仅需要从正态分布中采样就可以输出任意图片。\n\n","tags":["paper notes","generative model","VAE"]}]