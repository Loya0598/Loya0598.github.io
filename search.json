[{"title":"VAE","url":"/2020/06/18/VAE/","content":"\n## Paper: Auto-Encoding Variational Bayes\n\n### 论文链接：https://arxiv.org/pdf/1312.6114.pdf\n\n### 代码链接：https://github.com/bojone/vae\n\n### 阅读时间：2020.06.17\n\n------\n\n### 一、知识补充\n\n- 泛函：函数的函数；变分问题就是为了求解泛函问题的极值。\n- 题目中的泛函其实指的是KL散度作为loss找特定分布的时候用到了泛函的思想\n\n### 二、VAE 和 普通AutoEncoder的区别\n\n- 首先，我们考虑普通的autoencoder， 它的结构是通过对输入进行编码得到一个中间的隐层向量，然后希望通过解码隐层向量得到的输出能和输入的差别尽量小，通过这种编码解码的结构，以重构的差异作为损失来训练它的网络。\n- 但是这样做存在一个局限，==当我们想运用已经训练好的autoencoder模型时我们无法直接给定中间的隐层向量就让模型输出任意的图片==，因为我们不知道中间隐层向量的分布。\n- 所以VAE就是在encoder这个层面想了一个办法，尽量==让隐层向量的分布符合某个正态分布==，这样当我们已经知道了正态分布的均值和方差之后，我们就可以生成一个中间的隐层向量，然后利用它去生成任意的图片。\n\n### 三、VAE如何做（只考虑Encoder，因为我认为VAE仅在Encoder层面作出了很大改变）\n\n- 目标：P(Z | X) 趋向某种正态分布\n- 方法：\n  - Encoder分为两部分：均值Encoder和方差Encoder（方差这里做了一个log的处理，但对整体分析没有影响，所以暂时不考虑这一块）\n  - Encoder的Loss设计：输入的图片，输入均值和方差，利用KL散度去衡量这个均值和方差与$N(0, 1)$之间的差距。（这个意义上来说，对于任何分布的数据，我们都能将它变换到正态分布？这种理论依据是？）\n- 模型应用：\n  - 在对模型进行训练之后，我们仅需要从正态分布中采样就可以输出任意图片。\n\n","tags":["generative model"],"categories":["paper"]}]